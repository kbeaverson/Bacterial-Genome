{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15dc821-7108-46a9-bad5-4f6f59121f8d",
   "metadata": {},
   "source": [
    "# Workshop 5 - Model Building\n",
    "\n",
    "In this workshop we're going to leverage the features we generated last week and train a few different models. If you completed the assignment last week you'll already have a high level idea of the performance we should be aiming for when training our models.\n",
    "\n",
    "This tutorial focuses on a few different model architectures and how to set up k-fold cross validation.\n",
    "\n",
    "Lets run through the steps together (there are some questions and some blanks to fill in as we run through)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7140deb-bd7d-48db-9a5a-367700da47a3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d595ae-d425-4900-8b62-7136b25e777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327e0dd-fa16-4a1b-aea9-a2e7b2e17d01",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "\n",
    "For this workshop please download the latest:\n",
    "- `train_test_data` folder and put within `data/`\n",
    "\n",
    "Key for data:\n",
    "- train_pa_genes = presence absence binary features for training data\n",
    "- test_pa_genes = presence absence binary features for test data\n",
    "- train_kmers = kmer counts for training data\n",
    "- test_kmers = kmer counts for testing data\n",
    "- y_train = array of S/R target values\n",
    "- y_train_ids = array of genome_ids in order of y_train\n",
    "- y_test_ids = array of genome_ids in order of y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd3c81-1867-4819-b907-5f5816a41379",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 130\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the data needed for Workshop 5\n",
    "    \"\"\"\n",
    "    # Presence absence features\n",
    "    train_pa_genes = pd.read_csv('../data/train_test_data/train_pa_genes.csv').set_index('genome_id')\n",
    "    test_pa_genes = pd.read_csv('../data/train_test_data/test_pa_genes.csv').set_index('genome_id')\n",
    "    \n",
    "    # Load Kmer data\n",
    "    train_kmers = np.load('../data/train_test_data/train_kmers.npy', allow_pickle=True)\n",
    "    test_kmers = np.load('../data/train_test_data/test_kmers.npy', allow_pickle=True)\n",
    "\n",
    "    # Load target data & IDs\n",
    "    y_train = np.load('../data/train_test_data/y_train.npy', allow_pickle=True)\n",
    "    y_train_ids = np.load('../data/train_test_data/train_ids.npy', allow_pickle=True).astype(str)\n",
    "    y_test_ids = np.load('../data/train_test_data/test_ids.npy', allow_pickle=True).astype(str)\n",
    "\n",
    "    # Load raw gene data for optional neural network section\n",
    "    train_gene_alignment = pd.read_csv('../data/train_test_data/train_genes.csv')\n",
    "    \n",
    "    return train_pa_genes, test_pa_genes, train_kmers, test_kmers, y_train, y_train_ids, y_test_ids, train_gene_alignment\n",
    "\n",
    "train_pa_genes, test_pa_genes, X_train_kmers, X_test_kmers, y_train, y_train_ids, y_test_ids, train_gene_alignment = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354afc5-3d28-4f03-9c52-8dea927efed0",
   "metadata": {},
   "source": [
    "## 1. Linear Models\n",
    "\n",
    "For our first model we're going to try using a simple regression based model. The key limitation of regression is that it will only model linear combinations of our input features which may or may not be sufficient.\n",
    "\n",
    "If we wanted to use the linear model for inference (reviewing feature importances and understanding the impact of predictors on our response) we'd want to be much more careful about ensuring we're meeting the assumptions of linear regression (see this nice article: https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cf53c-3da6-4ad5-9d8c-451be0833bca",
   "metadata": {},
   "source": [
    "#### Check our data\n",
    "\n",
    "- Our target (response) is either S/R so we have a binary prediction\n",
    "- This means we'll need to use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb0f0f-7851-49b3-bce5-53bcf15a4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87593f7e-2738-40e5-9a8c-c5e7ddd4ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa_genes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88893fc-285e-4030-a1c1-66e55ab24688",
   "metadata": {},
   "source": [
    "#### Convert dataframes to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ce6b2-dd3b-4269-ab66-a8cd2bd9ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pa = np.array(train_pa_genes)\n",
    "X_test_pa = np.array(test_pa_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80397427-8b44-407c-8f0d-0339b224564c",
   "metadata": {},
   "source": [
    "#### Build Simple Logistic model\n",
    "\n",
    "Sklearn has a simple interface for building logistic models: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "By default this model has regularization, lets try with it off first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4b375-8d7b-47df-a937-4cf1b0a5fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = ---\n",
    "logistic_model.fit(---)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b966c-9440-4fd2-af22-2473f7ce3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_log_pa = logistic_model.predict(X_train_pa)\n",
    "sklearn.metrics.balanced_accuracy_score(y_train, y_pred_train_log_pa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72fd381-8924-4ac8-b623-e7ec3aaaf2c9",
   "metadata": {},
   "source": [
    "#### Try regularizing\n",
    "\n",
    "- Regularizing adds a penality to the loss function\n",
    "- The idea being that it will penalize the model for high weights and reduce overfitting to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53fa3e-b47d-4d5b-9560-5e3ec911eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = linear_model.LogisticRegression(max_iter=10000, penalty='l2')\n",
    "logistic_model.fit(X_train_pa, y_train.reshape(-1))\n",
    "\n",
    "y_pred_train_log_pa = logistic_model.predict(X_train_pa)\n",
    "sklearn.metrics.balanced_accuracy_score(y_train, y_pred_train_log_pa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f0f71-b67c-4cc9-8498-71de29db1fd0",
   "metadata": {},
   "source": [
    "<div class=\"question\" style=\"color: #534646; background-color: #ffdfa3; padding: 1px; border-radius: 5px;\">\n",
    "\n",
    "#### Q. What do we think of these balanced accuracy scores?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1a81d-bdc5-42c6-b28f-924882dca0f7",
   "metadata": {},
   "source": [
    "#### Finally we could also use Kmers in exactly the same way\n",
    "- Both are tabular datasets\n",
    "- Given we have a lot more kmer features this will take longer to train\n",
    "- You may also need more regularization to offset the additional N features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97105ae8-9136-4f66-ae12-95efad94e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_model = linear_model.LogisticRegression(penalty='l2')\n",
    "# logistic_model.fit(X_train_kmers, y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4c01f-54c6-4eb2-a193-385e5334e9e1",
   "metadata": {},
   "source": [
    "## 2. Tree Based Models\n",
    "\n",
    "Sklearn has a very similar interface for fitting tree based models.\n",
    "\n",
    "In this case we'll try:\n",
    "1. Simple decision tree\n",
    "2. Ensemble random forest method\n",
    "\n",
    "Tree based models are a great fit for binary feature data due to the successive decision making process but it will also work for both our tabular feature sets.\n",
    "\n",
    "In this case lets try using the kmer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98bd4a-b00e-4854-b9b4-fc65d738a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a decision tree classifier \n",
    "tree_model = tree.DecisionTreeClassifier(\n",
    "    ---\n",
    ")\n",
    "tree_model.fit(---)\n",
    "\n",
    "y_pred_train_tree_kmer = tree_model.predict(---)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cd773-7a17-4d23-9386-dfea7db5b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our balanced accuracy\n",
    "sklearn.metrics.balanced_accuracy_score(y_train, y_pred_train_tree_kmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57a08f-c057-4839-9812-3b22297aec2c",
   "metadata": {},
   "source": [
    "<div class=\"question\" style=\"color: #534646; background-color: #ffdfa3; padding: 1px; border-radius: 5px;\">\n",
    "\n",
    "#### Q. What would happen if we keep increasing max depth? Are there any alternatives?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1027764-54b9-4d16-af33-99e5ee8ff175",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c768be-eb7b-4f9e-a845-80fd61f1a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a decision tree classifier \n",
    "rf_model = ensemble.RandomForestClassifier(\n",
    "   ---\n",
    ")\n",
    "rf_model.fit(---)\n",
    "\n",
    "y_pred_train_rf_kmer = rf_model.predict(---)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e2333-c179-4642-814a-86175de52d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our balanced accuracy\n",
    "sklearn.metrics.balanced_accuracy_score(y_train, y_pred_train_rf_kmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fb59f-9328-43fd-8046-77d7c7e58296",
   "metadata": {},
   "source": [
    "<div class=\"question\" style=\"color: #534646; background-color: #ffdfa3; padding: 1px; border-radius: 5px;\">\n",
    "\n",
    "#### Q. Why do we see this change in train accuracy?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760aaf1f-1835-45cc-92bb-867a9eee552b",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting\n",
    "\n",
    "Gradient boosting is generally considered to be the best default choice for many predictive modeling problems from tabular data. It commonly comes out on top during Kaggle competitions for a wide array of datasets. \n",
    "\n",
    "The ideal model will always depend on the data you're using but gradient boosting is always a good place to start.\n",
    "\n",
    "In practice you might wish to use XGBOOST or LightGBM packages (feel free to install and use for your final project) but here we'll use the simple sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2e483-aa1a-4489-a522-732a9ba7a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_model = sklearn.ensemble.AdaBoostClassifier(\n",
    "    estimator = ---, # Can choose any simple base estimator\n",
    "    n_estimators = 10,\n",
    "    learning_rate = 2.0, # Another parameter to tune\n",
    "    algorithm=\"SAMME\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b650610-4981-44b5-abbb-9f83ba69d932",
   "metadata": {},
   "source": [
    "### Sample Weighting\n",
    "\n",
    "So far we've just fit basic default models, during your project you'll want to be more careful about tuning parameters and optimizing (we'll cover this next week).\n",
    "\n",
    "One approach that will be useful across many model types however is sample weighting:\n",
    "- We know our dataset is imbalanced\n",
    "- We wish to encourage the model to learn both classes\n",
    "- To do so we can upweight the minority class and downweight the majority class\n",
    "- Sklearn can do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303ccf1-588c-4587-be8e-8ba682963513",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = sklearn.utils.class_weight.compute_sample_weight(---)\n",
    "\n",
    "# Check the weights for a few samples\n",
    "pd.DataFrame(list(zip(y_train[0:10], sample_weights[0:10])), columns=['y_train', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf3222-5e05-4527-8450-67c3a54f3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the booster (train time is a bit too long to demo so lets take only 100 samples)\n",
    "X_train_kmers\n",
    "boost_model.fit(X_train_kmers[0:100], y_train[0:100].reshape(-1), sample_weight=sample_weights[0:100])\n",
    "\n",
    "y_pred_train_boost_kmer = boost_model.predict(X_train_kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb759612-71c7-49de-9a29-cc20d9a700ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our balanced accuracy\n",
    "sklearn.metrics.balanced_accuracy_score(y_train, y_pred_train_boost_kmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82b327-405f-4df4-bcb1-f290b42ca46c",
   "metadata": {},
   "source": [
    "## 4. Cross validation\n",
    "\n",
    "So far we've seen a few different interfaces for training various models but we've only been looking at the training data.\n",
    "\n",
    "For the test dataset we don't have access to the labels (they've been hidden as part of the final project) so what should we use to assess our models more fairly?\n",
    "\n",
    "This is where K-fold CV and validation data in general comes into play.\n",
    "\n",
    "We want to split our training data into train/validate where we hold out a portion of the data for checking model performance whilst tuning.\n",
    "\n",
    "As usual there are a lot of nice packages where this has already been implemented for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2c400-547d-48d3-afb1-7848ed0e0737",
   "metadata": {},
   "source": [
    "#### Train Test Split\n",
    "\n",
    "- If we just want a single split (one off)\n",
    "- This will randomly split up data and match the IDs between train and validate for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa27c4-c795-4226-804b-1548d8bb6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, X_validate_split, y_train_split, y_validate_split = sklearn.model_selection.train_test_split(\n",
    "    ---\n",
    "    random_state=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ffdf6-7e6c-48ca-9cb6-2a12232ed700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_split.shape, y_train_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f16f8-3c44-4aa6-963d-84c2ba2aaf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_validate_split.shape, y_validate_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c548ba-961d-421a-8dcf-181b3e1e3883",
   "metadata": {},
   "source": [
    "#### K-FOLD CV\n",
    "\n",
    "In reality we want to make multiple splits so we can train multiple models.\n",
    "\n",
    "This will allow to avoid overfitting to any specific split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dac647-f664-492e-b907-04f53b3ce06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "kfold = sklearn.model_selection.KFold(\n",
    "    ---\n",
    "    random_state = seed, # To ensure reproducible results\n",
    ")\n",
    "\n",
    "kfold_dfs = {}\n",
    "for --- in enumerate(kfold.split(X_train_pa)):\n",
    "    \n",
    "    # Can either train models directly here or save out the data for future training\n",
    "    kfold_dfs[i] = (X_train_pa[train_index], X_train_pa[val_index], y_train[train_index], y_train[val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c938b6-d378-4ac3-87be-06873336f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fold 0 X Train: \", kfold_dfs[0][0].shape)\n",
    "print(\"Fold 0 y Train: \", kfold_dfs[0][2].shape)\n",
    "print(\"Fold 0 X Validate: \", kfold_dfs[0][1].shape)\n",
    "print(\"Fold 0 y Validate: \", kfold_dfs[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5b784-2854-4896-9cb1-5c065e354bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fold 1 X Train: \", kfold_dfs[1][0].shape)\n",
    "print(\"Fold 1 y Train: \", kfold_dfs[1][2].shape)\n",
    "print(\"Fold 1 X Validate: \", kfold_dfs[1][1].shape)\n",
    "print(\"Fold 1 y Validate: \", kfold_dfs[1][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04179bd4-0b73-4027-b055-37bd43e9d263",
   "metadata": {},
   "source": [
    "#### Recommend stratifying on our target\n",
    "- When using this in your project it would be useful to also Stratify on the target variable\n",
    "- This ensures we have an even balance of S/R in each split\n",
    "- Avoids having any individual fold with an odd balance of S/R (e.g. missing any R examples)\n",
    "- You can use StratifiedKFold for this: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fa7ce-8c75-40d4-9914-82da7c0c52b9",
   "metadata": {},
   "source": [
    "## 5. [BONUS] Convolutional Neural Network\n",
    "\n",
    "If you're interested in trying to use the sequencing features + a CNN here is an example of both featurization and model training:\n",
    "\n",
    "- This particular model doesn't learn effectively from the data (it predicts majority class)\n",
    "- It's going to be challenging to get it to learn but will be an interesting task to try!\n",
    "- This can act as a starting point for future experimentation\n",
    "- Much of the work will need to be in the featurization step: how to combine genes\n",
    "\n",
    "I'm happy to schedule an office hour session to talk through if interested!\n",
    "\n",
    "You may wish to use google colab or another cloud based platform with scalable resources to ensure you can train the model without running out of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e8672-5cdb-4192-8e83-6a4a26b50858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take only the unique genes which weren't redundant (based on presence absence)\n",
    "# This is just to reduce the data size and make it easier to train, ideally we'd use all genes here\n",
    "subset_gene_data = train_gene_alignment[train_gene_alignment.res_gene.isin(set(train_pa_genes.columns))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36c1dc-18fe-47aa-a593-f90c07799285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find maximum length and set padding\n",
    "subset_gene_features = subset_gene_data.groupby('genome_id', sort=False)['ref_gene_str'].sum()  \n",
    "pad_char = 0\n",
    "max_length = np.max([len(x) for x in subset_gene_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c8172-b1a0-46ca-a3e6-168567710641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to featurize sequence data\n",
    "def encode_seq(seq):\n",
    "    label_enc = {'A':1, 'C':2, 'G':3, 'T':4}\n",
    "    return [label_enc.get(x.upper(), 5) for x in seq]\n",
    "\n",
    "def featurize_variant_sequences(variant_genes, amr_max_length, pad_char=0):\n",
    "    gene_features = variant_genes.groupby('genome_id', sort=False)['ref_gene_str'].sum()\n",
    "    gene_features = [encode_seq(x) for x in gene_features]\n",
    "    gene_features = keras.utils.pad_sequences(gene_features, maxlen=max_length, padding='post', value=pad_char)\n",
    "       \n",
    "    return gene_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ee58d-5e52-4145-b8c0-2220e191db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Featurize the data into our simple encoding\n",
    "X_seq = featurize_variant_sequences(subset_gene_data, max_length)\n",
    "X_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06907509-7977-40a1-aeb4-8a1b07838c62",
   "metadata": {},
   "source": [
    "#### Define a simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd97f09-58b1-4474-9092-352295a02e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN\n",
    "input_layer = keras.layers.Input(shape=(X_seq.shape[-1], 1))\n",
    "cnn_layer = keras.layers.Conv1D(\n",
    "    20,\n",
    "    11,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation='relu'\n",
    ")(input_layer)\n",
    "pool = keras.layers.MaxPool1D(pool_size=3)(cnn_layer)\n",
    "cnn_layer2 = keras.layers.Conv1D(\n",
    "    30,\n",
    "    15,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation='relu'\n",
    ")(pool)\n",
    "pool2 = keras.layers.MaxPool1D(pool_size=5)(cnn_layer2)\n",
    "cnn_layer3 = keras.layers.Conv1D(\n",
    "    50,\n",
    "    21,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation='relu'\n",
    ")(pool2)\n",
    "pool3 = keras.layers.MaxPool1D(pool_size=7)(cnn_layer2)\n",
    "final_pool = keras.layers.Flatten()(pool3)\n",
    "dense = keras.layers.Dense(20, activation='relu')(final_pool)\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "cnn = keras.Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e404b-3666-4484-939f-639699c0e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model structure\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c73a7-edf8-4cd6-b929-1c6a3fe2f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model and select optimizer\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32d07b-e8df-4ae6-b5eb-d91f37dbccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode y_train to numeric binary 1/0 from S/R\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_binary = le.fit_transform(y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19826bd0-403e-42ea-8062-627ba402e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = cnn.fit(X_seq, y_train_binary, validation_split=0.2, batch_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023fe87-7f1b-4d94-9c46-913bd876f16c",
   "metadata": {},
   "source": [
    "### This looks like it's started to learn something!\n",
    "\n",
    "It's clearly lagging behind in terms of validation accuracy though and is potentially overfitting!\n",
    "\n",
    "A few ideas:\n",
    "- Use an encoding layer rather than simple numeric encoding\n",
    "- Use all the genes!\n",
    "- Pad the genes individually and then concatenate them to better preserve position information better\n",
    "- Try different pooling strategies\n",
    "- Use dropout for regularizing\n",
    "\n",
    "You may use this architecture of feel free to start from scratch using your own CNN approach\n",
    "\n",
    "You could also try RNN or other convolution approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab6e61-757b-497e-945b-b4e9f8130256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
