{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define a method to load the kmer data for our Histogram Gradient Boosting model, selected due to large dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_kmer():\n",
    "    train_kmers = np.load('data/train_test_data/train_kmers.npy', allow_pickle=True)\n",
    "    test_kmers = np.load('data/train_test_data/test_kmers.npy', allow_pickle=True)\n",
    "\n",
    "    # Load target data & IDs\n",
    "    y_train = np.load('data/train_test_data/y_train.npy', allow_pickle=True)\n",
    "    y_train_ids = np.load('data/train_test_data/train_ids.npy', allow_pickle=True).astype(str)\n",
    "    y_test_ids = np.load('data/train_test_data/test_ids.npy', allow_pickle=True).astype(str)\n",
    "\n",
    "    return train_kmers, test_kmers, y_train, y_train_ids, y_test_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load kmer data, define variables for kfold split/model hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kmer data and convert to numpy arrays for use in models\n",
    "train_kmers, test_kmers, y_train, y_train_ids, y_test_ids = load_data_kmer()\n",
    "X_train = np.array(train_kmers)\n",
    "X_test = np.array(test_kmers)\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "# Seed for consistency in testing\n",
    "seed = 73\n",
    "\n",
    "# Kfold variables\n",
    "k = 5\n",
    "n_iter = 50\n",
    "cv = 10\n",
    "\n",
    "# Model variables\n",
    "n_estimators_low = 1\n",
    "n_estimators_high = 50\n",
    "k_best = 2000\n",
    "model_performance = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build stratified kfold (given the lopsided-ness of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = sklearn.model_selection.StratifiedKFold(\n",
    "    n_splits = k,\n",
    "    shuffle = True,\n",
    "    random_state = seed,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define method to select features using chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train, X_test, y_train):\n",
    "    selector = SelectKBest(score_func=chi2, k=k_best)\n",
    "    X_train_new = selector.fit_transform(X_train, y_train)\n",
    "    X_test_new = selector.transform(X_test)\n",
    "\n",
    "    return X_train_new, X_test_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define a method to build an HGB model, using chi2 feature selection to reduce computation costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gb_model(X_train: np.array, y_train: np.array, performance_dict: dict, kfold: KFold, n_estimators_low: int, n_estimators_high: int, n_iter: int, cv: int, seed: int, k_best: int = 200):\n",
    "    for i, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "        print(f\"Starting Outer fold {i}\")\n",
    "        X_train_outer, X_val_outer, y_train_outer, y_val_outer  = (\n",
    "            X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
    "        )\n",
    "\n",
    "        # Select k best features to reduce cost\n",
    "        X_train_outer, X_val_outer = select_features(X_train=X_train_outer, y_train=y_train_outer, X_test=X_val_outer)\n",
    "        print(\"K Best selected.\")\n",
    "\n",
    "        gbc_random_cv = sklearn.model_selection.RandomizedSearchCV(\n",
    "            estimator = ensemble.HistGradientBoostingClassifier(random_state=seed),\n",
    "            param_distributions = {\n",
    "                \"learning_rate\": stats.loguniform(0.001, 1),\n",
    "                \"max_iter\": stats.randint(n_estimators_low, n_estimators_high),\n",
    "                \"l2_regularization\": stats.loguniform(1e-5, 10)\n",
    "            },\n",
    "            cv = cv,\n",
    "            scoring = sklearn.metrics.make_scorer(sklearn.metrics.balanced_accuracy_score),\n",
    "            random_state = seed,\n",
    "            n_jobs = -1,\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        print(f\"Fitting model for fold {i}\")\n",
    "        gbc_random_cv.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "        # Assess the best model using the outer validation data\n",
    "        print(f\"Assessing model performance for fold {i}\")\n",
    "        y_pred_outer_gbc = gbc_random_cv.predict(X_val_outer)\n",
    "        performance_dict[i] = sklearn.metrics.confusion_matrix(y_val_outer, y_pred_outer_gbc, labels=[\"S\",\"R\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train models using nested K-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gb_model(\n",
    "    X_train = X_train,\n",
    "    y_train = y_train,\n",
    "    performance_dict = model_performance,\n",
    "    kfold = kfold,\n",
    "    n_estimators_low = n_estimators_low,\n",
    "    n_estimators_high = n_estimators_high,\n",
    "    n_iter = n_iter,\n",
    "    cv = cv,\n",
    "    seed = seed,\n",
    "    k_best = k_best,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Combine all data across all folds into one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_matrix_kmer = np.mean(list(model_performance.values()), axis = 0)\n",
    "pd.DataFrame(data = combined_matrix_kmer, index = [\"S\", \"R\"], columns = [\"S\", \"R\"])\n",
    "print(\"Kmer performance:\")\n",
    "print(combined_matrix_kmer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Define a function to calculate the confidence intervals for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normal_confidence_intervals(confusion_matrices_list):\n",
    "    accuracy_list = []\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    bal_acc_list = []\n",
    "\n",
    "    for confusion_matrix in confusion_matrices_list:\n",
    "        sensitivity = confusion_matrix[1,1] / confusion_matrix[1,:].sum()\n",
    "        specificity = confusion_matrix[0,0] / confusion_matrix[0,:].sum()\n",
    "\n",
    "        accuracy_list.append((confusion_matrix[0,0] + confusion_matrix[1,1]) / confusion_matrix.sum())\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        bal_acc_list.append(np.mean([sensitivity, specificity]))\n",
    "\n",
    "    location, std = np.mean(accuracy_list), np.std(accuracy_list)\n",
    "    accuracy_ci = stats.norm.interval(0.95, loc = location, scale = std)\n",
    "\n",
    "    location, std = np.mean(sensitivity_list), np.std(sensitivity_list)\n",
    "    sensitivity_ci = stats.norm.interval(0.95, loc = location, scale = std)\n",
    "\n",
    "    location, std = np.mean(specificity_list), np.std(specificity_list)\n",
    "    specificity_ci = stats.norm.interval(0.95, loc = location, scale = std)\n",
    "\n",
    "    location, std = np.mean(bal_acc_list), np.std(bal_acc_list)\n",
    "    bal_acc_ci = stats.norm.interval(0.95, loc = location, scale = std)\n",
    "\n",
    "    return accuracy_ci, sensitivity_ci, specificity_ci, bal_acc_ci"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Calculate confidence intervals for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ci, sens_ci, spec_ci, ba_ci = calculate_normal_confidence_intervals(list(model_performance.values()))\n",
    "print(f\"Accuracy confidence interval: {acc_ci}\")\n",
    "print(f\"Sensitivity confidence interval: {sens_ci}\")\n",
    "print(f\"Specificity confidence interval: {spec_ci}\")\n",
    "print(f\"Balanced accuracy confidence interval: {ba_ci}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. After finalizing the variable ranges, build a final model using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = select_features(X_train=X_train, X_test=X_test, y_train=y_train)\n",
    "\n",
    "print(\"Training final model\")\n",
    "gb_final_cv = sklearn.model_selection.RandomizedSearchCV(\n",
    "    estimator = ensemble.HistGradientBoostingClassifier(random_state=seed),\n",
    "    param_distributions = {\n",
    "        \"learning_rate\": stats.loguniform(0.001, 1),\n",
    "        \"max_iter\": stats.randint(n_estimators_low, n_estimators_high),\n",
    "        \"l2_regularization\": stats.loguniform(1e-5, 10)\n",
    "    },\n",
    "    cv = cv,\n",
    "    scoring = sklearn.metrics.make_scorer(sklearn.metrics.balanced_accuracy_score),\n",
    "    random_state = seed,\n",
    "    n_jobs = 1,\n",
    "    n_iter = 5,\n",
    "    verbose = 1,\n",
    ")\n",
    "\n",
    "# Fit the final model\n",
    "print(\"Fitting final cv model\")\n",
    "gb_final_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get optimal parameters\n",
    "print(\"Getting optimal parameters\")\n",
    "final_lr = gb_final_cv.best_estimator_.get_params()[\"learning_rate\"]\n",
    "final_max_iter = gb_final_cv.best_estimator_.get_params()[\"max_iter\"]\n",
    "final_l2_regularization = gb_final_cv.best_estimator_.get_params()[\"l2_regularization\"]\n",
    "\n",
    "print(f\"Final learning rate: {final_lr}\")\n",
    "print(f\"Final max iterations: {final_max_iter}\")\n",
    "print(f\"Final l2 regularization: {final_l2_regularization}\")\n",
    "\n",
    "print(\"Building final model\")\n",
    "# Build final model\n",
    "final_model = ensemble.HistGradientBoostingClassifier(\n",
    "    learning_rate = final_lr,\n",
    "    max_iter = final_max_iter,\n",
    "    l2_regularization = final_l2_regularization,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Fit the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Predict and output results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = final_model.predict(X_test)\n",
    "final_predictions = pd.DataFrame(data = {\"genome_id\":y_test_ids, \"y_pred\":y_pred_test})\n",
    "final_predictions.to_csv('kmer_gb_hist_stratified.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etbg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
